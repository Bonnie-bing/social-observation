---
title: "Learning_GLMM"
author: "Chunliang Feng (SCNU)"
date: June 21, 2022
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
#rm(list=ls())
require("knitr") 
opts_chunk$set(tidy = FALSE, warning = FALSE, message = FALSE, cache = FALSE)
#load libraries
suppressMessages(library("tidyverse")) # to organize data
suppressMessages(library("afex"))  # using 'all_fit' to find appropriate optimizer; and 'mixed' for significance test
suppressMessages(library("lme4"))  # to fit data with mixed models
suppressMessages(library("emmeans"))  # emmeans is needed for follow-up tests 
suppressMessages(library("multcomp")) # for advanced control for multiple testing/Type 1 errors. using 'update' for post-hoc comp
suppressMessages(library("ggeffects")) # for plotting (G)LMM results
suppressMessages(library('car')) # setting labeled contrasts
suppressMessages(library("ggplot2")) # for plotting
suppressMessages(library("ggsci")) # for setting scientific journal color palettes 
suppressMessages(library("gridExtra")) # for arranging the position of figures
suppressMessages(library("ggpubr")) # using the ggexport
suppressMessages(library("sjPlot")) # using the plot_model, to show random and fixed effects.
suppressMessages(library("glmmTMB")) #required by plot_model when plotting random effects
suppressMessages(library("performance")) # model assumptions & performance, to use check_model for model assumptions
suppressMessages(library("RePsychLing")) #using the rePCA to determine redundant random effects
suppressMessages(library("ggthemes"))#provide theme for plot
suppressMessages(library("patchwork")) 

select <- dplyr::select # to avoid clashes with the MASS package


```


0. Task description
```{r task description}
# In this task, 47 participants completed 5 blocks of reinforcement learning task in two social contexts: alone or the presence of others (as denoted by the variable 'appearance'). 

# Each block consisted of 6 new and distinct pictures in 3 feedback validity (as denoted by variable 'prob' or 'prob2'): 100%, 80%, and 52%. In particular, one picture was 100% associated with left key, one was 100% associated with right key. Likewise, two pictures were respectively 80% associated with left and right keys. Finally, two pictures were respectively 52% associated with left and right keys. Each block was split into two bins (as denoted by the variable 'bin'), that is, early or late learning stage 

# On each trial, participants were asked to respond with left or right key at the presence of a given picture. Their response was followed with a feedback according to participant's response and feedback validity. Therefore, participants could improve their choice according to choice and outcome in a trial-and-error manner.

# More details on the task and data analysis could be found in the 'Task design and Data analysis.docx' file or the following reference:
#Eppinger, B., Kray, J., Mock, B., & Mecklinger, A. (2008). Better or worse than expected? Aging, learning, and the ERN. Neuropsychologia, 46(2), 521-539.

# This analysis focused on participant's reaction times (as denoted by variable 'RT'). A LMM was constructed as detailed below. In particular, we asked the question of whether participant's RT was modulated by experimental factors manipulated in the current study, that is, social context (appearance), feedback validity (prob2) and learning stage (bin).


```



1. load data
```{r loaddata}
raw_data                     <- read.csv(file="all_data.csv",head=TRUE) #read data
data_df                      <- raw_data
```

2.0 data descriptions
```{r data descriptions}
#the following key variables are relevant to the current analysis
#DV: RT.
#IV: 1. appearance(0=alone;1=other);2. prob2(1=high;2=medium;3=low);3. bin(1,2). 
#random/grouping variable: subid
```

2.1 data cleaning
```{r data cleaning}
#1. remove 2301~2307, incorrect RT recording for these participants
data_df        <- data_df %>% filter(subid != 2301 & subid != 2302 & subid != 2303 & subid != 2304 & subid != 2305 & subid != 2306 & subid != 2307)
data_df        <- data_df %>% filter(ACC != 0) #excluding wrong responses
data_df        <- data_df %>% filter(RT > 0.1) # excluding fast responses, i.e., less than 0.1 s
data_df$RT     <- data_df$RT*1000 #transfer to ms


data_df$subid                <- factor(data_df$subid)
data_df$appearance           <- factor(data_df$appearance)
data_df$prob                 <- factor(data_df$prob)
data_df                      <- data_df %>% mutate(prob2 = as.factor(ifelse(prob == "1",'High',ifelse(prob == "2",'Medium','Low')))) #transfer numbers to string
data_df$prob2                <- factor(data_df$prob2,levels=c('High','Medium','Low')) #reorder the levels; prob2 was computed to avoid errors in emmeans package.
data_df$bin                  <- factor(data_df$bin)

#distribution of the data
hist(data_df$RT)
#test the distributions that best fit your data
#https://ase.tufts.edu/bugs/guide/assets/mixed_model_guide.html
#Say you've decided you want to run a mixed model. The next thing you have to do is find a probability distribution that best fits your data. There are many ways to test this, but I'll show you one. This method requires the packages 'car' and 'MASS'. Note that the negative binomial and gamma distributions can only handle positive numbers, and the Poisson distribution can only handle positive whole numbers. The binomial and Poisson distributions are different from the others because they are discrete rather than continuous, which means they quantify distinct, countable events or the probability of these events. Now let's find a fitting distribution
qqp(data_df$RT, "norm") # norm dis
# lnorm means lognormal
qqp(data_df$RT, "lnorm")

# qqp requires estimates of the parameters of the negative binomial, Poisson
# and gamma distributions. You can generate estimates using the fitdistr
# function. Save the output and extract the estimates of each parameter as I
# have shown below.
##The following binom and poission does not fit for RT data, because they are discrete distributions.
# nbinom <- fitdistr(data_df$RT, "Negative Binomial")
# qqp(data_df$RT, "nbinom", size = nbinom$estimate[[1]], mu = nbinom$estimate[[2]])
# 
# poisson <- fitdistr(data_df$RT, "Poisson")
# qqp(data_df$RT, "pois", poisson$estimate)

gamma <- fitdistr(data_df$RT, "gamma")
qqp(data_df$RT, "gamma", shape = gamma$estimate[[1]], rate = gamma$estimate[[2]])

#Note: Check out the plots I've generated using qqp. The y axis represents the observations and the x axis represents the quantiles modeled by the distribution. The solid red line represents a perfect distribution fit and the dashed red lines are the confidence intervals of the perfect distribution fit. You want to pick the distribution for which the largest number of observations falls between the dashed lines.


```

2.2 describing data
```{r desrciping data}
####################################################1. how many subjects remained##################################################################
data_df %>% group_by(subid) %>% summarize() %>% nrow()

###############################################2. how many trials left for each cell##############################################################
tn <- data_df %>% group_by(subid,prob,bin) %>% summarize(trialn=length(subid)) %>% ungroup()

min(tn$trialn)# check if any condition has too few trials

##############################################3. check the levels of the factors####################################################################
levels(data_df$appearance)
levels(data_df$prob)
levels(data_df$prob2)
levels(data_df$bin)

```

2.3 re-coding and scaling predictors
```{r recoding and scaling}
#######################################################1. re-coding categorical predictors###################################################
contrasts(data_df$appearance)                <- contr.Sum(levels(data_df$appearance)) # must be Sum instead of sum
contrasts(data_df$prob)                      <- contr.Sum(levels(data_df$prob))
contrasts(data_df$prob2)                     <- contr.Sum(levels(data_df$prob2))
contrasts(data_df$bin)                       <- contr.Sum(levels(data_df$bin))

#check the new coding approach
contrasts(data_df$appearance)
contrasts(data_df$prob)
contrasts(data_df$prob2)
contrasts(data_df$bin)

```


3.1. fit an initial model, to see if it converges
```{r ini model}
m1.ini <- lmer(RT ~ 1 + appearance * prob2 *bin + (1 + prob2 *bin | subid), data = data_df)
summary(m1.ini)

```
The results show that this model coverges, but it is a singular fit. So, we will use the all_fit function to find optimizers that work.


3.2. fixing the singular fit
```{r fixing model}
all_fit(m1.ini)  # find working optimizers
```

The results show that the 'Nelder_Mead' optimizer works, so we will use this optimizer. In addition, we add two other control parameters to avoid calculating derives and to increase the number of iterations



3.3. fit a new model with control parameters
```{r full model}
m1.full <- lmer(RT ~ 1 + appearance * prob2 *bin + (1 + prob2 *bin | subid), data = data_df,control = lmerControl(optimizer = 'Nelder_Mead',calc.derivs = F,optCtrl = list(maxfun = 2e5)))
save(m1.full, file = "m1_full_RT.RData")
summary(m1.full)
```

This model works, so we will do the significance test with this model.


4. significance test
```{r sig test}
#The reduced model should be the same as the full model in terms of random effects and control parameters etc.
#That is, everything else except the interested effect should be the same between full and reduced models.
load("m1_full_RT.RData")

RT.sig1 <- anova(m1.full,type = 3, test = 'F', ddf='Satterthwaite')  # F test

# here we do not use the LRT, since it results in strange results due to singular fit in the reduced model.
RT.sig2 <- mixed(RT ~ 1 + appearance * prob2 *bin + 
                      (1 + prob2 *bin | subid), 
                data=data_df, method='LRT', # could be 'PB'
                       check_contrasts = F,control = lmerControl(optimizer = 'Nelder_Mead',calc.derivs = F,optCtrl = list(maxfun = 2e5)))

RT.sig1
RT.sig2
save(RT.sig1, file = "RT_sig.RData")

```

5. post-hoc comparisons
```{r post-hoc comparisons}
#https://cran.r-project.org/web/packages/afex/vignettes/afex_mixed_example.html
#https://zhuanlan.zhihu.com/p/63092231?utm_source=wechat_session&utm_medium=social&utm_oi=669244116261539840&s_r=0
#https://cran.r-project.org/web/packages/emmeans/vignettes/interactions.html
load("m1_full_RT.RData")
###########################################################for RT######################################################################
emm_options(lmer.df = "asymptotic") # also possible: 'satterthwaite', 'kenward-roger' for LMMs but not GLMMs
# for continuous predictors,condition on prob2
# for the two-way interaction of bin:prob2
emmip(m1.full, prob2 ~ bin) #simple plotting
#emm_acc.int2 <- emmeans(m1.full, "bin", by = "prob2")
emm_acc.int2 <- emmeans(m1.full, ~bin | prob2) #same as above
emm_acc.int2
update(pairs(emm_acc.int2), by = NULL, adjust = "holm") # using the FDR corrections here


emm_acc.int22 <- emmeans(m1.full, ~prob2|bin)
emm_acc.int22
update(pairs(emm_acc.int22), by = NULL, adjust = "holm") # using the FDR corrections here


```


6. model diagnoses
```{r model diagnoses,, fig.height = 6, fig.width = 10}

############################1. plot the fitted value and the residuals#################################################
#plot(m1) # no specific pattern is good
plot(m1.full) # no specific pattern is good

##################################################2. plot the QQ plot#################################################
#qqnorm(resid(m1))
qqnorm(resid(m1.full))

################################################3. check the model assumptions in one line#########################################
#check_model(m1)
check_model(m1.full)


##########################################4. check out random effects and plot them##############################################
cfs <- coef(m1.full)
var(cfs$subid$`appearance[S.0]`) ## to calculate the variance of fixed effects (should be 0), just as an example here, can do for others

#plot with the plot_model:http://www.strengejacke.de/sjPlot/reference/plot_model.html
#fixed effects,the same as summary funnction, should be careful, the interpretation of these effects depends on your coding scheme!!!
#plot_model(m1.full)+ ylim(-.05, .05) #see also https://github.com/strengejacke/sjPlot/issues/440
plot_model(m1.full)

#random effects
RT.re  <- plot_model(m1.full, type = "re", grid=FALSE)  #

grid.arrange(RT.re[[1]],RT.re[[2]],RT.re[[3]],RT.re[[4]],RT.re[[5]],RT.re[[6]],nrow=2,ncol=3)


```



7. plotting results
```{r sig plotting results}
#https://stackoverflow.com/questions/69948052/plot-generalized-linear-mixed-models-glmms-mixture-of-categorical-and-numeric
#https://strengejacke.github.io/ggeffects/articles/practical_logisticmixedmodel.html
# see also ggeffect manual
load("m1_full_RT.RData")

################################################################### generating the marginal effects####################################################
mydf <- ggemmeans(m1.full, terms = c("bin", "prob2")) ##do not use 'ggpredict', otherwise, do not consistent with post-hoc comparisions, see also https://strengejacke.github.io/ggeffects/articles/technical_differencepredictemmeans.html
mydf <- mydf %>% mutate(x_lab = as.factor(ifelse(x == "1",'Early','Late')))

windowsFonts(N=windowsFont("Times New Roman"),A=windowsFont("Arial")) #Custom Fonts
pd <- position_dodge(0.2) #https://stackoverflow.com/questions/39533456/how-to-jitter-both-geom-line-and-geom-point-by-the-same-magnitude

pic1 <- ggplot(mydf, aes(x=x_lab, y=predicted,group=group,color = group,fill=group)) +
    #note:The layer placed first will be placed at the bottom. In this case,the line will be covered by points.  
    geom_line(linetype='dashed',size=1,position = pd)+
    geom_pointrange(aes(ymin=conf.low, ymax=conf.high), shape=21, fatten = 5, size = 1.5,position = pd)+
    #The adjustment of line size is replaced by 'linewidth' in the new version of ggplot
    scale_y_continuous(limits = c(380,560),breaks = seq(from=380,to=560,by=30),expand = c(0, 0))+
    #https://stackoverflow.com/questions/13701347/force-the-origin-to-start-at-0
    labs(x = "Learning Stage", y = "RT (ms)")+
    #http://www.sthda.com/english/wiki/ggplot2-colors-how-to-change-colors-automatically-and-manually
    scale_colour_manual(values=c('#4DBBD6','#039F87','#E64B35'),name='Validity',labels=c('high', 'medium','low'))+
    scale_fill_manual(values=c('#4DBBD6','#039F87','#E64B35'),name='Validity',labels=c('high', 'medium','low'))+
    theme_classic()+
    theme(legend.position="top",
          legend.title = element_text(size=20,colour='black',family="A"),
          legend.text = element_text(size=20,colour='black',family="A"),
          axis.ticks.length = unit(0.1, "cm"),
          axis.text.x = element_text(size=20,colour='black',family="A"),
          axis.text.y = element_text(size=20,colour='black',family="A"),
          axis.title= element_text(size=24,colour='black',family="A"))
pic1
ggexport(pic1,filename = "p1_test1.png",width=2400,height=1600,res = 300)
ggsave(pic1, filename = "p1_test2.png", dpi = 300,width=2400,height=1600,units = "px")#same as above, but export pic with better anti-aliasing effect


############Using Custom Function#####################
#When the aesthetic scheme is determined, the custom functions helps to plot different data more easily
#You can flexibly decide the input of your function according to your own needs(The input is the parameter you may want to modify)
effectplot <- function(d,ylimits,ybreaks,xlabs){
fig <- ggplot(d, aes(x=x_lab, y=predicted,group=group,color = group,fill=group)) +
    #note:The layer placed first will be placed at the bottom. In this case,the line will be covered by points.  
    geom_line(linetype='dashed',size=1,position = pd)+
    geom_pointrange(aes(ymin=conf.low, ymax=conf.high), shape=21, fatten = 5, size = 1.5,position = pd)+
    #The adjustment of line size is replaced by 'linewidth' in the new version of ggplot
    scale_y_continuous(limits = ylimits,breaks = ybreaks,expand = c(0, 0))+
    #https://stackoverflow.com/questions/13701347/force-the-origin-to-start-at-0
    labs(x = xlabs, y = "RT (ms)")+
    #http://www.sthda.com/english/wiki/ggplot2-colors-how-to-change-colors-automatically-and-manually
    scale_colour_manual(values=c('#4DBBD6','#039F87','#E64B35'),name='Validity',labels=c('high', 'medium','low'))+
    scale_fill_manual(values=c('#4DBBD6','#039F87','#E64B35'),name='Validity',labels=c('high', 'medium','low'))+
    theme_classic()+
    theme(legend.position="top",
          legend.title = element_text(size=20,colour='black',family="A"),
          legend.text = element_text(size=20,colour='black',family="A"),
          axis.ticks.length = unit(0.1, "cm"),
          axis.text.x = element_text(size=20,colour='black',family="A"),
          axis.text.y = element_text(size=20,colour='black',family="A"),
          axis.title= element_text(size=24,colour='black',family="A"))
}

mydf2 <- ggemmeans(m1.full, terms = c("appearance", "prob2"))
mydf2 <- mydf2 %>% mutate(x_lab = as.factor(ifelse(x == "0",'Alone','Audience')))

#check range of y value
min(mydf$conf.low)
max(mydf$conf.high)
#https://www.roelpeters.be/add-confidence-interval-line-ggplot/

pic2 <- effectplot(mydf2,c(380,560),seq(from=380,to=560,by=30),xlabs='Context')
pic2
ggsave(pic2, filename = "p2.png", dpi = 300,width=2400,height=1600,units = "px")


```

